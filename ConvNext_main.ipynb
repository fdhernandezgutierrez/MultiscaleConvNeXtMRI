{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1743497392691,
     "user": {
      "displayName": "Fernando Hernandez",
      "userId": "03780154294249447225"
     },
     "user_tz": 360
    },
    "id": "Tx0V84Ev7Myj"
   },
   "outputs": [],
   "source": [
    "#os.chdir('/content/drive/MyDrive/Escuela/Doctorado/Projects/ConvNext/ConvNext_V2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 21 12:32:05 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.65.06              Driver Version: 580.65.06      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 ...    Off |   00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   41C    P8             10W /  220W |      18MiB /  12282MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2687      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1743497391362,
     "user": {
      "displayName": "Fernando Hernandez",
      "userId": "03780154294249447225"
     },
     "user_tz": 360
    },
    "id": "DvcraTuJ5Gcr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()  \n",
    "torch.cuda.memory_reserved()  \n",
    "import zipfile\n",
    "import os\n",
    "import nibabel as nib\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "import wandb\n",
    "from torchvision import models\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rKxwLzT630o"
   },
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4733,
     "status": "ok",
     "timestamp": 1743497404363,
     "user": {
      "displayName": "Fernando Hernandez",
      "userId": "03780154294249447225"
     },
     "user_tz": 360
    },
    "id": "d_cU7nu17ShJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 1, 512, 512])\n",
      "Path BrATS2021: BrATS2021/\n",
      "Path BrATS2023: BrATS2023/\n"
     ]
    }
   ],
   "source": [
    "%run src/ConvNext_models.ipynb\n",
    "%run src/metrics.ipynb\n",
    "%run src/hg_lossfunctions.ipynb\n",
    "%run src/trainer.ipynb\n",
    "%run src/BratsDataloader.ipynb\n",
    "%run src/Optimizer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "torch.Size([6, 1, 512, 512])\n",
      "torch.Size([6, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "base_path =\"../BRATS2024/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512))  # Cambia al tamaño deseado\n",
    "    #transforms.Normalize(mean=[0.5], std=[0.5]) # For -1 a 1\n",
    "])\n",
    "\n",
    "# Crear el dataset y el DataLoader\n",
    "dataset = BRATSDataset_2(base_path, img_transform=transform, mask_transform = transform)\n",
    "\n",
    "train_size = int(0.7 * len(dataset))  \n",
    "val_size   = int(0.15 * len(dataset))  \n",
    "test_size  = len(dataset) - train_size - val_size  \n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "BATCH_SIZE = 6\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size   = BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size  = BATCH_SIZE, shuffle=False)\n",
    "print(np.shape(train_loader))\n",
    "for img, mask in train_loader:\n",
    "    print(np.shape(img))\n",
    "    print(np.shape(mask))\n",
    "    break\n",
    "\n",
    "def get_model_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "\n",
    "def get_model_memory_size(model):\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    return param_size\n",
    "def get_model_flops(model, input_size=(1, 512, 512), device='cuda'):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    total_flops = 0\n",
    "    total_params = sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1743497411395,
     "user": {
      "displayName": "Fernando Hernandez",
      "userId": "03780154294249447225"
     },
     "user_tz": 360
    },
    "id": "tghYONMV7j6b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Model Name: ConvNeXtSegmentationMF\n",
      "------------------------ OPtimizer Name: AdamW\n",
      "------------------------ Loss Name: BCE\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfd-hernandezgutierrez\u001b[0m (\u001b[33mhgfernandod\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/fernando/KINGSTON/to_GitHub/wandb/run-20251021_123219-ncqx3b5d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hgfernandod/ConvNext_V2_RTX4070Super_Brats2024_/runs/ncqx3b5d' target=\"_blank\">blooming-breeze-10</a></strong> to <a href='https://wandb.ai/hgfernandod/ConvNext_V2_RTX4070Super_Brats2024_' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hgfernandod/ConvNext_V2_RTX4070Super_Brats2024_' target=\"_blank\">https://wandb.ai/hgfernandod/ConvNext_V2_RTX4070Super_Brats2024_</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hgfernandod/ConvNext_V2_RTX4070Super_Brats2024_/runs/ncqx3b5d' target=\"_blank\">https://wandb.ai/hgfernandod/ConvNext_V2_RTX4070Super_Brats2024_/runs/ncqx3b5d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] - Train Loss: 0.2878, Val Loss: 0.1716\n",
      "New best model saved with Val Loss: 0.1716\n",
      "Epoch [2/100] - Train Loss: 0.1427, Val Loss: 0.1482\n",
      "New best model saved with Val Loss: 0.1482\n",
      "Epoch [3/100] - Train Loss: 0.1259, Val Loss: 0.0975\n",
      "New best model saved with Val Loss: 0.0975\n",
      "Epoch [4/100] - Train Loss: 0.1096, Val Loss: 0.1156\n",
      "Epoch [5/100] - Train Loss: 0.1118, Val Loss: 0.0864\n",
      "New best model saved with Val Loss: 0.0864\n",
      "Epoch [6/100] - Train Loss: 0.0918, Val Loss: 0.0774\n",
      "New best model saved with Val Loss: 0.0774\n",
      "Epoch [7/100] - Train Loss: 0.0738, Val Loss: 0.0531\n",
      "New best model saved with Val Loss: 0.0531\n",
      "Epoch [8/100] - Train Loss: 0.0730, Val Loss: 0.0612\n",
      "Epoch [9/100] - Train Loss: 0.0758, Val Loss: 0.0947\n",
      "Epoch [10/100] - Train Loss: 0.0563, Val Loss: 0.0478\n",
      "New best model saved with Val Loss: 0.0478\n",
      "Epoch [11/100] - Train Loss: 0.0471, Val Loss: 0.0707\n",
      "Epoch [12/100] - Train Loss: 0.0506, Val Loss: 0.0519\n",
      "Epoch [13/100] - Train Loss: 0.0493, Val Loss: 0.0447\n",
      "New best model saved with Val Loss: 0.0447\n",
      "Epoch [14/100] - Train Loss: 0.0431, Val Loss: 0.0457\n",
      "Epoch [15/100] - Train Loss: 0.0422, Val Loss: 0.0788\n",
      "Epoch [16/100] - Train Loss: 0.0475, Val Loss: 0.0612\n",
      "Epoch [17/100] - Train Loss: 0.0492, Val Loss: 0.0524\n",
      "Epoch [18/100] - Train Loss: 0.0596, Val Loss: 0.1097\n",
      "Epoch [19/100] - Train Loss: 0.1011, Val Loss: 0.0728\n",
      "Epoch [20/100] - Train Loss: 0.0720, Val Loss: 0.1014\n",
      "Epoch [21/100] - Train Loss: 0.0515, Val Loss: 0.0463\n",
      "Epoch [22/100] - Train Loss: 0.0366, Val Loss: 0.0419\n",
      "New best model saved with Val Loss: 0.0419\n",
      "Epoch [23/100] - Train Loss: 0.0322, Val Loss: 0.0560\n",
      "Epoch [24/100] - Train Loss: 0.0354, Val Loss: 0.0414\n",
      "New best model saved with Val Loss: 0.0414\n",
      "Epoch [25/100] - Train Loss: 0.0280, Val Loss: 0.0437\n",
      "Epoch [26/100] - Train Loss: 0.0225, Val Loss: 0.0427\n",
      "Epoch [27/100] - Train Loss: 0.0216, Val Loss: 0.0359\n",
      "New best model saved with Val Loss: 0.0359\n",
      "Epoch [28/100] - Train Loss: 0.0213, Val Loss: 0.0424\n",
      "Epoch [29/100] - Train Loss: 0.0201, Val Loss: 0.0397\n",
      "Epoch [30/100] - Train Loss: 0.0190, Val Loss: 0.0402\n",
      "Epoch [31/100] - Train Loss: 0.0194, Val Loss: 0.0396\n",
      "Epoch [32/100] - Train Loss: 0.0172, Val Loss: 0.0395\n",
      "Epoch [33/100] - Train Loss: 0.0158, Val Loss: 0.0337\n",
      "New best model saved with Val Loss: 0.0337\n",
      "Epoch [34/100] - Train Loss: 0.0189, Val Loss: 0.0561\n",
      "Epoch [35/100] - Train Loss: 0.0223, Val Loss: 0.0473\n",
      "Epoch [36/100] - Train Loss: 0.0188, Val Loss: 0.0448\n",
      "Epoch [37/100] - Train Loss: 0.0164, Val Loss: 0.0470\n",
      "Epoch [38/100] - Train Loss: 0.0160, Val Loss: 0.0588\n",
      "Epoch [39/100] - Train Loss: 0.0185, Val Loss: 0.0394\n",
      "Epoch [40/100] - Train Loss: 0.0166, Val Loss: 0.0416\n",
      "Epoch [41/100] - Train Loss: 0.0163, Val Loss: 0.1234\n",
      "Epoch [42/100] - Train Loss: 0.0507, Val Loss: 0.0636\n",
      "Epoch [43/100] - Train Loss: 0.0420, Val Loss: 0.0413\n",
      "Epoch [44/100] - Train Loss: 0.0385, Val Loss: 0.0513\n",
      "Epoch [45/100] - Train Loss: 0.0430, Val Loss: 0.0606\n",
      "Epoch [46/100] - Train Loss: 0.0337, Val Loss: 0.0492\n",
      "Epoch [47/100] - Train Loss: 0.0240, Val Loss: 0.0436\n",
      "Epoch [48/100] - Train Loss: 0.0193, Val Loss: 0.0529\n",
      "Epoch [49/100] - Train Loss: 0.0174, Val Loss: 0.0357\n",
      "Epoch [50/100] - Train Loss: 0.0175, Val Loss: 0.0386\n",
      "Epoch [51/100] - Train Loss: 0.0147, Val Loss: 0.0409\n",
      "Epoch [52/100] - Train Loss: 0.0141, Val Loss: 0.0522\n",
      "Epoch [53/100] - Train Loss: 0.0147, Val Loss: 0.0415\n",
      "Epoch [54/100] - Train Loss: 0.0140, Val Loss: 0.0429\n",
      "Epoch [55/100] - Train Loss: 0.0140, Val Loss: 0.0458\n",
      "Epoch [56/100] - Train Loss: 0.0129, Val Loss: 0.0401\n",
      "Epoch [57/100] - Train Loss: 0.0126, Val Loss: 0.0389\n",
      "Epoch [58/100] - Train Loss: 0.0119, Val Loss: 0.0406\n",
      "Epoch [59/100] - Train Loss: 0.0113, Val Loss: 0.0351\n",
      "Epoch [60/100] - Train Loss: 0.0115, Val Loss: 0.0410\n",
      "Epoch [61/100] - Train Loss: 0.0113, Val Loss: 0.0347\n",
      "Epoch [62/100] - Train Loss: 0.0105, Val Loss: 0.0365\n",
      "Epoch [63/100] - Train Loss: 0.0107, Val Loss: 0.0390\n",
      "Epoch [64/100] - Train Loss: 0.0107, Val Loss: 0.0450\n",
      "Epoch [65/100] - Train Loss: 0.0107, Val Loss: 0.0506\n",
      "Epoch [66/100] - Train Loss: 0.0105, Val Loss: 0.0407\n",
      "Epoch [67/100] - Train Loss: 0.0105, Val Loss: 0.0480\n",
      "Epoch [68/100] - Train Loss: 0.0102, Val Loss: 0.0407\n",
      "Epoch [69/100] - Train Loss: 0.0107, Val Loss: 0.0463\n",
      "Epoch [70/100] - Train Loss: 0.0101, Val Loss: 0.0405\n",
      "Epoch [71/100] - Train Loss: 0.0101, Val Loss: 0.0447\n",
      "Epoch [72/100] - Train Loss: 0.0097, Val Loss: 0.0445\n",
      "Epoch [73/100] - Train Loss: 0.0094, Val Loss: 0.0383\n",
      "Epoch [74/100] - Train Loss: 0.0096, Val Loss: 0.0630\n",
      "Epoch [75/100] - Train Loss: 0.0168, Val Loss: 0.0682\n",
      "Epoch [76/100] - Train Loss: 0.0280, Val Loss: 0.0532\n",
      "Epoch [77/100] - Train Loss: 0.0245, Val Loss: 0.0602\n",
      "Epoch [78/100] - Train Loss: 0.0230, Val Loss: 0.0739\n",
      "Epoch [79/100] - Train Loss: 0.0511, Val Loss: 0.0851\n",
      "Epoch [80/100] - Train Loss: 0.0565, Val Loss: 0.0560\n",
      "Epoch [81/100] - Train Loss: 0.0350, Val Loss: 0.0494\n",
      "Epoch [82/100] - Train Loss: 0.0283, Val Loss: 0.0596\n",
      "Epoch [83/100] - Train Loss: 0.0203, Val Loss: 0.0424\n",
      "Epoch [84/100] - Train Loss: 0.0195, Val Loss: 0.0393\n",
      "Epoch [85/100] - Train Loss: 0.0165, Val Loss: 0.0511\n",
      "Epoch [86/100] - Train Loss: 0.0132, Val Loss: 0.0445\n",
      "Epoch [87/100] - Train Loss: 0.0121, Val Loss: 0.0492\n",
      "Epoch [88/100] - Train Loss: 0.0120, Val Loss: 0.0366\n",
      "Epoch [89/100] - Train Loss: 0.0117, Val Loss: 0.0385\n",
      "Epoch [90/100] - Train Loss: 0.0107, Val Loss: 0.0429\n",
      "Epoch [91/100] - Train Loss: 0.0102, Val Loss: 0.0453\n",
      "Epoch [92/100] - Train Loss: 0.0102, Val Loss: 0.0405\n",
      "Epoch [93/100] - Train Loss: 0.0103, Val Loss: 0.0396\n",
      "Epoch [94/100] - Train Loss: 0.0095, Val Loss: 0.0437\n",
      "Epoch [95/100] - Train Loss: 0.0094, Val Loss: 0.0427\n",
      "Epoch [96/100] - Train Loss: 0.0097, Val Loss: 0.0453\n",
      "Epoch [97/100] - Train Loss: 0.0097, Val Loss: 0.0477\n",
      "Epoch [98/100] - Train Loss: 0.0092, Val Loss: 0.0453\n",
      "Epoch [99/100] - Train Loss: 0.0091, Val Loss: 0.0435\n",
      "Epoch [100/100] - Train Loss: 0.0086, Val Loss: 0.0422\n",
      "Training completed.\n",
      "Loaded best model weights.\n",
      "Resultados guardados en PDF: BRATS2024_ConvNeXtSegmentationMF_AdamW_BCE.pdf\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted: '/media/fernando/KINGSTON/to_GitHub/BRATS2024_ConvNeXtSegmentationMF_AdamW_BCE.pdf' -> '/media/fernando/KINGSTON/to_GitHub/wandb/run-20251021_123219-ncqx3b5d/files/BRATS2024_ConvNeXtSegmentationMF_AdamW_BCE.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Visualize results\u001b[39;00m\n\u001b[1;32m     73\u001b[0m final_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBRATS2024_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mmodel_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m optimizer_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mlosses_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 74\u001b[0m Conv_Next_trainer\u001b[38;5;241m.\u001b[39mvisualize_results(L_Net_images, L_Net_targets, L_Net_preds, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, save_path\u001b[38;5;241m=\u001b[39m final_file)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m th \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.8\u001b[39m]:\n\u001b[1;32m     76\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m Conv_Next_trainer\u001b[38;5;241m.\u001b[39mevaluate_metrics(threshold\u001b[38;5;241m=\u001b[39mth)\n",
      "File \u001b[0;32m/tmp/ipykernel_778176/4173788224.py:214\u001b[0m, in \u001b[0;36mUNetTrainer.visualize_results\u001b[0;34m(images, targets, preds, num_samples, save_path)\u001b[0m\n\u001b[1;32m    212\u001b[0m pdf\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResultados guardados en PDF: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 214\u001b[0m wandb\u001b[38;5;241m.\u001b[39msave(save_path)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/wandb/sdk/wandb_run.py:399\u001b[0m, in \u001b[0;36m_log_to_run.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attach_id\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wb_logging\u001b[38;5;241m.\u001b[39mlog_to_run(run_id):\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/wandb/sdk/wandb_run.py:457\u001b[0m, in \u001b[0;36m_raise_if_finished.<locals>.wrapper_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m: Run, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_finished\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 457\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    459\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    460\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is finished. The call to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please make sure that you are using an active run.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    463\u001b[0m     )\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UsageError(message)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/wandb/sdk/wandb_run.py:444\u001b[0m, in \u001b[0;36m_attach.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m         _is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/wandb/sdk/wandb_run.py:2126\u001b[0m, in \u001b[0;36mRun.save\u001b[0;34m(self, glob_str, base_path, policy)\u001b[0m\n\u001b[1;32m   2120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2121\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnly \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlive\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnow\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m policies are currently supported.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2122\u001b[0m     )\n\u001b[1;32m   2124\u001b[0m resolved_base_path \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPurePath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(base_path))\n\u001b[0;32m-> 2126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save(\n\u001b[1;32m   2127\u001b[0m     resolved_glob_path,\n\u001b[1;32m   2128\u001b[0m     resolved_base_path,\n\u001b[1;32m   2129\u001b[0m     policy,\n\u001b[1;32m   2130\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/wandb/sdk/wandb_run.py:2185\u001b[0m, in \u001b[0;36mRun._save\u001b[0;34m(self, glob_path, base_path, policy)\u001b[0m\n\u001b[1;32m   2182\u001b[0m     \u001b[38;5;66;03m# Delete the symlink if it exists.\u001b[39;00m\n\u001b[1;32m   2183\u001b[0m     target_path\u001b[38;5;241m.\u001b[39munlink(missing_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2185\u001b[0m     target_path\u001b[38;5;241m.\u001b[39msymlink_to(source_path)\n\u001b[1;32m   2187\u001b[0m \u001b[38;5;66;03m# Inform users that new files aren't detected automatically.\u001b[39;00m\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m had_symlinked_files \u001b[38;5;129;01mand\u001b[39;00m is_star_glob:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/pathlib/_local.py:789\u001b[0m, in \u001b[0;36mPath.symlink_to\u001b[0;34m(self, target, target_is_directory)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msymlink_to\u001b[39m(\u001b[38;5;28mself\u001b[39m, target, target_is_directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    785\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;124;03m    Make this path a symlink pointing to the target path.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;124;03m    Note the order of arguments (link, target) is the reverse of os.symlink.\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     os\u001b[38;5;241m.\u001b[39msymlink(target, \u001b[38;5;28mself\u001b[39m, target_is_directory)\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted: '/media/fernando/KINGSTON/to_GitHub/BRATS2024_ConvNeXtSegmentationMF_AdamW_BCE.pdf' -> '/media/fernando/KINGSTON/to_GitHub/wandb/run-20251021_123219-ncqx3b5d/files/BRATS2024_ConvNeXtSegmentationMF_AdamW_BCE.pdf'"
     ]
    }
   ],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes     = 1  # Ejemplo: edema, realce, necrosis, etc.\n",
    "ConvNext_models = {  #'HyperNetSegmentation'    : HyperNetSegmentation,\n",
    "                     'ConvNeXtSegmentationMF'  : ConvNeXtSegmentationMF, # dió IoU: 0.8\n",
    "                     #'ConvNeXtUNet'            :prueba_borrar ConvNeXtUNet,\n",
    "                     #'ConvNeXtUNet_B'          : ConvNeXtUNet_B,\n",
    "                     #ConvNext_Base'           : ConvNeXtSegmentation\n",
    "                     #'ConvNeXtSegmentationRA'  : ConvNeXtSegmentationRA #use highg resources\n",
    "}\n",
    "\n",
    "optimizers      =  { \n",
    "                     #'Adam'     : lambda model: optim.Adam(model.parameters(), lr = 0.001),\n",
    "                     'AdamW'    : lambda model: optim.AdamW(model.parameters(), lr=0.001),\n",
    "                     #'CRTAdamW' : lambda model:CRTAdamW(model.parameters(), lr = 0.001),\n",
    "                  }\n",
    "\n",
    "functions_losses = { #'BCEDiceLoss': BCEDiceLoss(),\n",
    "                     #'DSC': DiceLoss(),\n",
    "                     'BCE':nn.BCEWithLogitsLoss()\n",
    "                   }\n",
    "\n",
    "for model_name, model_class in ConvNext_models.items():      # Models \n",
    "    for optimizer_name, optimizer_class in optimizers.items(): # Optimizers\n",
    "        for losses_name, losses_class in functions_losses.items(): # Losses\n",
    "            print('------------------------ Model Name:', model_name)\n",
    "            print('------------------------ OPtimizer Name:', optimizer_name)\n",
    "            print('------------------------ Loss Name:', losses_name)\n",
    "\n",
    "            if model_name =='ConvNeXtUNet' or  model_name == 'ConvNeXtRAUNet' or model_name == 'ConvNeXtMFRAUNet':\n",
    "                model = model_class(num_classes, backbone_type='base')\n",
    "            else:\n",
    "                model       =  model_class(num_classes)   #ConvNeXtSegmentation(num_classes)\n",
    "            \n",
    "            model       = model.to(device)  # Si estás usando una GPU\n",
    "            criterion   =  losses_class #nn.BCEWithLogitsLoss()  # CrossEntropyLoss()\n",
    "            optimizer   =  optimizer_class(model)# CRTAdamW(model.parameters(), lr = 0.001)#optim.AdamW(model.parameters(), lr=0.001)# CRTAdamW\n",
    "            num_epochs  = 100\n",
    "        \n",
    "            Metrics = []\n",
    "            Conv_Next_trainer = UNetTrainer(\n",
    "                    model        = model,\n",
    "                    device       = device,\n",
    "                    optimizer    = optimizer,\n",
    "                    criterion    = criterion,\n",
    "                    train_loader = train_loader,\n",
    "                    val_loader   = val_loader,\n",
    "                    test_loader  = test_loader,\n",
    "                    epochs       = num_epochs,\n",
    "                    filename_model = model_name + '.pth'\n",
    "                )\n",
    "            total_params      = get_model_parameters(model)\n",
    "            model_memory_size = get_model_memory_size(model)/ (1024 ** 2)\n",
    "            FLOps             = get_model_flops(model)\n",
    "            print(FLOps)\n",
    "            wandb.init(project=\"ConvNext_V2_RTX4070Super_Brats2024_\", config={\n",
    "                'Dataset'       : base_path, \n",
    "                'device'        : device,\n",
    "                'model name'    : model_name,\n",
    "                'epochs'        : num_epochs,\n",
    "                'batch size'    : BATCH_SIZE,\n",
    "                'loss_function' : losses_name, #criterion,\n",
    "                'optimizer'     : optimizer_name,#optimizer,\n",
    "                'total params'  :total_params,\n",
    "                'Model Size (MB)'   : model_memory_size\n",
    "            })\n",
    "            # Start training\n",
    "            Conv_Next_trainer.train()\n",
    "            # Test the model if needed\n",
    "            L_Net_images, L_Net_preds, L_Net_targets = Conv_Next_trainer.test()\n",
    "            # Visualize results\n",
    "            final_file = \"BRATS2024_\"+model_name+\"_\"+ optimizer_name+\"_\"+losses_name+'.pdf'\n",
    "            Conv_Next_trainer.visualize_results(L_Net_images, L_Net_targets, L_Net_preds, num_samples=10, save_path= final_file)\n",
    "            for th in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:\n",
    "                metrics = Conv_Next_trainer.evaluate_metrics(threshold=th)\n",
    "                Metrics.append(metrics)\n",
    "        \n",
    "            wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase de pruebas usando K-Fold y graficar distribución de violin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes     = 1  # Ejemplo: edema, realce, necrosis, etc.\n",
    "ConvNext_models = {  #'HyperNetSegmentation'    : HyperNetSegmentation,\n",
    "                     'ConvNeXtSegmentationMF'  : ConvNeXtSegmentationMF#, # dió IoU: 0.8\n",
    "                     #'ConvNeXtUNet'            : ConvNeXtUNet,\n",
    "                     #'ConvNeXtUNet_B'          : ConvNeXtUNet_B,\n",
    "                     #'ConvNext_Base'           : ConvNeXtSegmentation\n",
    "                     #'ConvNeXtSegmentationRA'  : ConvNeXtSegmentationRA #use highg resources\n",
    "}\n",
    "\n",
    "optimizers      =  { #'CRTAdamW' : lambda model:CRTAdamW(model.parameters(), lr = 0.001),\n",
    "                     'Adam'     : lambda model: optim.Adam(model.parameters(), lr = 0.001),\n",
    "                     'AdamW'    : lambda model: optim.AdamW(model.parameters(), lr=0.001)\n",
    "                    }\n",
    "\n",
    "functions_losses = { 'BCEDiceLoss': BCEDiceLoss(),\n",
    "                     'DSC': DiceLoss(),\n",
    "                     'BCE':nn.BCEWithLogitsLoss()\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "def train_models_violin(\n",
    "    ConvNext_models,\n",
    "    optimizers,\n",
    "    functions_losses,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    num_classes=1,\n",
    "    device=None,\n",
    "    num_epochs=100,\n",
    "    BATCH_SIZE=4,\n",
    "    use_kfold=False,\n",
    "    k=5,\n",
    "    metrics_to_plot=None   # <- lista opcional de métricas a graficar\n",
    "):\n",
    "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataset = train_loader.dataset\n",
    "\n",
    "    all_metrics_data = []\n",
    "\n",
    "    for model_name, model_class in ConvNext_models.items():\n",
    "        for optimizer_name, optimizer_class in optimizers.items():\n",
    "            for losses_name, losses_class in functions_losses.items():\n",
    "                \n",
    "                print('------------------------ Model:', model_name)\n",
    "                print('------------------------ Optimizer:', optimizer_name)\n",
    "                print('------------------------ Loss:', losses_name)\n",
    "\n",
    "                metrics_per_model = []\n",
    "\n",
    "                if use_kfold:\n",
    "                    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "                    fold_iter = enumerate(kf.split(dataset))\n",
    "                else:\n",
    "                    fold_iter = [(0, (range(len(dataset)), range(len(val_loader.dataset))))]\n",
    "\n",
    "                for fold, (train_idx, val_idx) in fold_iter:\n",
    "                    print(f\"===== Fold {fold+1} =====\")\n",
    "\n",
    "                    if use_kfold:\n",
    "                        train_subset = Subset(dataset, train_idx)\n",
    "                        val_subset   = Subset(dataset, val_idx)\n",
    "                        train_loader_fold = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "                        val_loader_fold   = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "                    else:\n",
    "                        train_loader_fold = train_loader\n",
    "                        val_loader_fold   = val_loader\n",
    "\n",
    "                    # Instanciar modelo\n",
    "                    if model_name in ['ConvNeXtUNet', 'ConvNeXtRAUNet', 'ConvNeXtMFRAUNet']:\n",
    "                        model = model_class(num_classes, backbone_type='base')\n",
    "                    else:\n",
    "                        model = model_class(num_classes)\n",
    "                    model = model.to(device)\n",
    "\n",
    "                    criterion = losses_class\n",
    "                    optimizer = optimizer_class(model)\n",
    "\n",
    "                    trainer = UNetTrainer(\n",
    "                        model        = model,\n",
    "                        device       = device,\n",
    "                        optimizer    = optimizer,\n",
    "                        criterion    = criterion,\n",
    "                        train_loader = train_loader_fold,\n",
    "                        val_loader   = val_loader_fold,\n",
    "                        test_loader  = test_loader,\n",
    "                        epochs       = num_epochs,\n",
    "                        filename_model = f\"{model_name}_fold{fold+1}.pth\"\n",
    "                    )\n",
    "\n",
    "                    wandb.init(project=\"ConvNext_V2_Kfold_RTX4070Super\", config={\n",
    "                        'Dataset'       : 'CustomDataset', \n",
    "                        'device'        : device,\n",
    "                        'model name'    : model_name,\n",
    "                        'epochs'        : num_epochs,\n",
    "                        'batch size'    : BATCH_SIZE,\n",
    "                        'loss_function' : losses_name,\n",
    "                        'optimizer'     : optimizer_name\n",
    "                    })\n",
    "\n",
    "                    trainer.train()\n",
    "                    L_Net_images, L_Net_preds, L_Net_targets = trainer.test()\n",
    "                    # Visualize results\n",
    "                    final_file = model_name+\"_\"+ optimizer_name+\"_\"+losses_name+\"_k_\"+fold+'.pdf'\n",
    "                    trainer.visualize_results(L_Net_images, L_Net_targets, L_Net_preds, num_samples=5, save_path= final_file)\n",
    "                    metrics_fold = trainer.evaluate_metrics(threshold=0.2)\n",
    "\n",
    "                    # Guardar métricas\n",
    "                    for metric_name, value in metrics_fold.items():\n",
    "                        metrics_per_model.append({\n",
    "                            \"Model\": model_name,\n",
    "                            \"Optimizer\": optimizer_name,\n",
    "                            \"Loss\": losses_name,\n",
    "                            \"Fold\": fold+1,\n",
    "                            \"Metric\": metric_name,\n",
    "                            \"Value\": value\n",
    "                        })\n",
    "                        all_metrics_data.append({\n",
    "                            \"Model\": model_name,\n",
    "                            \"Optimizer\": optimizer_name,\n",
    "                            \"Loss\": losses_name,\n",
    "                            \"Fold\": fold+1,\n",
    "                            \"Metric\": metric_name,\n",
    "                            \"Value\": value\n",
    "                        })\n",
    "\n",
    "                    wandb.finish()\n",
    "\n",
    "                # Crear DataFrame solo para este modelo y configuración\n",
    "                df_model = pd.DataFrame(metrics_per_model)\n",
    "\n",
    "                # Filtrar solo métricas deseadas si se pasa la lista\n",
    "                if metrics_to_plot is not None:\n",
    "                    df_model = df_model[df_model['Metric'].isin(metrics_to_plot)]\n",
    "\n",
    "                # Violin plot\n",
    "                plt.figure(figsize=(12,6))\n",
    "                sns.boxplot(\n",
    "                    x=\"Metric\",\n",
    "                    y=\"Value\",\n",
    "                    hue=\"Loss\",   # opcional: hue=\"Optimizer\"\n",
    "                    data=df_model\n",
    "                )\n",
    "                #sns.violinplot(\n",
    "                #    x     = \"Metric\",\n",
    "                #    y     = \"Value\",\n",
    "                #    hue   = \"Loss\",   # opcional: hue=\"Optimizer\"\n",
    "                #    data  = df_model,\n",
    "                #    split = True,\n",
    "                #    inner = None, #quartile\"\n",
    "                #    bw    = 0.5,           # ajusta el ancho del KDE (kernel density)\n",
    "                #    cut   = 0\n",
    "                #)\n",
    "                plt.title(f\"Distribución de métricas para {model_name} ({optimizer_name}, {losses_name})\")\n",
    "                plt.ylabel(\"Valor de la Métrica\")\n",
    "                plt.xlabel(\"Métricas\")\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.legend(title=\"Loss\")\n",
    "                plt.show()\n",
    "\n",
    "    return pd.DataFrame(all_metrics_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = train_models_violin(\n",
    "    ConvNext_models=ConvNext_models,\n",
    "    optimizers=optimizers,\n",
    "    functions_losses=functions_losses,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_classes=1,\n",
    "    device=device,\n",
    "    num_epochs=100,\n",
    "    BATCH_SIZE=6,\n",
    "    use_kfold=True,\n",
    "    k=5,\n",
    "    metrics_to_plot=[\"Dice Coefficient\", \"IoU Score\", \"Sensitivity\", \"Specificity\",'Accuracy','F1-Score']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMRLfJ7ghOc7/qQYb4g/BmD",
   "gpuType": "T4",
   "mount_file_id": "1S0uizbAOlP8EdEVIeuDO0N1Ap-co15qR",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
