{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876ab60-d2e8-4de2-ab90-dc53c78bc156",
   "metadata": {
    "id": "0876ab60-d2e8-4de2-ab90-dc53c78bc156"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "class SegmentationEvaluator:\n",
    "    def __init__(self, model, test_loader, threshold=0.6, device=None):\n",
    "        self.model = model.to(device)\n",
    "        self.test_loader = test_loader\n",
    "        self.threshold = threshold\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Diccionario de métricas\n",
    "        self.metrics_funcs = {\n",
    "            \"Dice Coefficient\": self.dice_coefficient,\n",
    "            \"IoU Score\": self.iou_score,\n",
    "            \"Precision\": self.precision,\n",
    "            \"Sensitivity (Recall)\": self.recall,\n",
    "            \"Specificity\": self.specificity,\n",
    "            \"Accuracy\": self.accuracy,\n",
    "            \"F1-Score\": self.f1_score,\n",
    "            #\"AUC\": self.auc_score\n",
    "            \"Hausdorff\":self.hausdorff_distance\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def dice_coefficient(preds, labels):\n",
    "        preds = preds.float()\n",
    "        labels = labels.float()\n",
    "        intersection = (preds * labels).sum()\n",
    "        union = preds.sum() + labels.sum()\n",
    "        return (2.0 * intersection / (union + 1e-8)).item()\n",
    "\n",
    "    @staticmethod\n",
    "    def iou_score(preds, labels):\n",
    "        preds = (preds).float()\n",
    "        labels = labels.float()\n",
    "        intersection = (preds * labels).sum()\n",
    "        union = preds.sum() + labels.sum() - intersection\n",
    "        return (intersection / (union + 1e-8)).item()\n",
    "\n",
    "    @staticmethod\n",
    "    def precision(preds, labels):\n",
    "        preds = preds.float()\n",
    "        labels = labels.float()\n",
    "        tp = (preds * labels).sum()\n",
    "        fp = (preds * (1 - labels)).sum()\n",
    "        return (tp / (tp + fp + 1e-8)).item()\n",
    "\n",
    "    @staticmethod\n",
    "    def recall(preds, labels):\n",
    "        \"\"\"\n",
    "        Calcula la sensibilidad (recall).\n",
    "        \"\"\"\n",
    "        preds = preds.float()\n",
    "        labels = labels.float()\n",
    "        tp = (preds * labels).sum()\n",
    "        fn = ((1 - preds) * labels).sum()\n",
    "        return (tp / (tp + fn + 1e-8)).item()\n",
    "\n",
    "    @staticmethod\n",
    "    def specificity(preds, labels):\n",
    "        preds = preds.float()\n",
    "        labels = labels.float()\n",
    "        tn = ((1 - preds) * (1 - labels)).sum()\n",
    "        fp = (preds * (1 - labels)).sum()\n",
    "        return (tn / (tn + fp + 1e-8)).item()\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(preds, labels):\n",
    "        preds = preds.float()\n",
    "        labels = labels.float()\n",
    "        correct = (preds == labels).sum()\n",
    "        total = labels.numel()\n",
    "        return (correct / total).item()\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_score(preds, labels):\n",
    "        \"\"\"\n",
    "        Calcula el F1-Score basado en precisión y recall.\n",
    "        \"\"\"\n",
    "        precision = SegmentationEvaluator.precision(preds, labels)\n",
    "        recall = SegmentationEvaluator.recall(preds, labels)\n",
    "        return 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def auc_score(preds_prob, labels):\n",
    "        \"\"\"\n",
    "        Calcula AUC a nivel de imagen o batch.\n",
    "        preds_prob: probabilidades (sigmoid)\n",
    "        labels: binarios\n",
    "        \"\"\"\n",
    "        preds = preds_prob.view(-1).cpu().numpy()\n",
    "        labels = labels.view(-1).cpu().numpy()\n",
    "    \n",
    "        # Verificar que haya al menos un pixel positivo y uno negativo\n",
    "        if np.any(labels == 1) and np.any(labels == 0):\n",
    "            return roc_auc_score(labels, preds)\n",
    "        else:\n",
    "            return np.nan  # Ignorar batch si no hay clases completas\n",
    "    \n",
    "    @staticmethod\n",
    "    def hausdorff_distance(preds, labels):\n",
    "        \"\"\"\n",
    "        Calcula la distancia de Hausdorff entre dos máscaras binarias.\n",
    "        preds y labels: tensores binarios (0 o 1) de forma [H, W]\n",
    "        \"\"\"\n",
    "        preds = preds.cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "    \n",
    "        # Obtener las coordenadas de los píxeles donde hay 1\n",
    "        pred_points = np.argwhere(preds == 1)\n",
    "        label_points = np.argwhere(labels == 1)\n",
    "    \n",
    "        if len(pred_points) == 0 or len(label_points) == 0:\n",
    "            return float('inf')  # Si alguna máscara está vacía\n",
    "    \n",
    "        # Distancias dirigidas\n",
    "        d_pred_label = directed_hausdorff(pred_points, label_points)[0]\n",
    "        d_label_pred = directed_hausdorff(label_points, pred_points)[0]\n",
    "    \n",
    "        # Hausdorff simétrica\n",
    "        return max(d_pred_label, d_label_pred)\n",
    "\n",
    "\n",
    "    def evaluate(self, visualize=False, num_images=2, save_path=None):\n",
    "        self.model.eval()\n",
    "        metrics = {key: 0 for key in self.metrics_funcs.keys()}\n",
    "        num_batches = len(self.test_loader)\n",
    "\n",
    "        all_images = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        print(\"Segmentation Evaluator/evaluate using threshold: \", self.threshold)\n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                preds_prob = torch.sigmoid(outputs)\n",
    "                preds_bin = (outputs > self.threshold).float()\n",
    "                \n",
    "                #plt.imshow(outputs[0,...].cpu(), cmap='gray')\n",
    "                #plt.show()\n",
    "                \n",
    "                for key, func in self.metrics_funcs.items():\n",
    "                    if key in ['FPS', 'latency']:\n",
    "                        continue\n",
    "                    if key == \"AUC\":\n",
    "                        metrics[key] += func(outputs, labels)  # AUC usa las predicciones continuas\n",
    "                    else:\n",
    "                        metrics[key] += func(preds_bin, labels)\n",
    "\n",
    "                if visualize:\n",
    "                    all_images.append(images.cpu())\n",
    "                    all_preds.append(preds_bin.cpu())\n",
    "                    all_labels.append(labels.cpu())\n",
    "\n",
    "        metrics = {k: v / num_batches for k, v in metrics.items()}\n",
    "        fps,ms_per_img=  fps_on_loader(self.model, self.test_loader) #se modificpo\n",
    "        avg_fps, std_fps, avg_latency, std_latency = measure_fps_repetitions(self.model, self.test_loader) # se modifico\n",
    "        \n",
    "        inference_time={\n",
    "            'FPS'         : fps,\n",
    "            'ms_per_img'  : ms_per_img,\n",
    "            'avg_fps'     : avg_fps,\n",
    "            'std_fps'     : std_fps,\n",
    "            'avg_latency' : avg_latency,\n",
    "            'std_latency' : std_latency\n",
    "        }\n",
    "        if visualize:\n",
    "            self.visualize_results(all_images, all_preds, all_labels, num_images, save_path)\n",
    "\n",
    "        metrics.update(inference_time)\n",
    "        print(metrics)\n",
    "        print(inference_time)\n",
    "        wandb.log(metrics)\n",
    "        return metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_results(images, preds, labels, num_images=2, save_path=None):\n",
    "        images = torch.cat(images)[:num_images]\n",
    "        preds = torch.cat(preds)[:num_images]\n",
    "        labels = torch.cat(labels)[:num_images]\n",
    "        \n",
    "        num_rows = num_images\n",
    "        plt.figure(figsize=(10, 3 * num_rows))\n",
    "        \n",
    "        for i in range(num_images):\n",
    "            image = images[i].squeeze(0).numpy()  # Remove channel dimension\n",
    "            pred = preds[i].squeeze(0).numpy()\n",
    "            label = labels[i].squeeze(0).numpy()\n",
    "        \n",
    "            plt.subplot(num_rows, 3, i * 3 + 1)\n",
    "            plt.imshow(image, cmap=\"gray\")\n",
    "            plt.title(\"Input Image\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(num_rows, 3, i * 3 + 2)\n",
    "            plt.imshow(pred, cmap=\"gray\")\n",
    "            plt.title(\"Predicted Mask\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(num_rows, 3, i * 3 + 3)\n",
    "            plt.imshow(label, cmap=\"gray\")\n",
    "            plt.title(\"Ground Truth Mask\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            plt.savefig(save_path)\n",
    "            print(f\"Imagen guardada en: {save_path}\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e7d3cf-073a-4e8f-ac57-60ffd5ab6794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fps_on_loader(model, test_loader, device=\"cuda\", warmup_batches=3):\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    model.to(device)\n",
    "\n",
    "    # ---------- WARM-UP ----------\n",
    "    for i, (x, _) in enumerate(test_loader):\n",
    "        if i == warmup_batches:\n",
    "            break\n",
    "        _ = model(x.to(device, non_blocking=True))\n",
    "\n",
    "    # ---------- MEDICIÓN ----------\n",
    "    total_imgs = 0\n",
    "    if device.startswith(\"cuda\") and torch.cuda.is_available():\n",
    "        starter = torch.cuda.Event(enable_timing=True)\n",
    "        ender   = torch.cuda.Event(enable_timing=True)\n",
    "        elapsed_ms = 0.0\n",
    "\n",
    "        for x, _ in test_loader:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            torch.cuda.synchronize()\n",
    "            starter.record()\n",
    "\n",
    "            _ = model(x)\n",
    "\n",
    "            ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed_ms += starter.elapsed_time(ender)\n",
    "            total_imgs += x.size(0)\n",
    "    else:  #  CPU (o GPU sin eventos)\n",
    "        tic = time.perf_counter()\n",
    "        for x, _ in test_loader:\n",
    "            x = x.to(device)\n",
    "            _ = model(x)\n",
    "            total_imgs += x.size(0)\n",
    "        elapsed_ms = (time.perf_counter() - tic) * 1_000\n",
    "\n",
    "    fps = 1_000 * total_imgs / elapsed_ms\n",
    "    ms_per_img = elapsed_ms / total_imgs\n",
    "    return fps, ms_per_img\n",
    "\n",
    "def measure_fps_repetitions(model, test_loader, device=\"cuda\", warmup_batches=3, repetitions=5):\n",
    "    fps_values = []\n",
    "    latency_values = []\n",
    "\n",
    "    # Realizar varias repeticiones\n",
    "    for _ in range(repetitions):\n",
    "        fps, latency = fps_on_loader(model, test_loader, device, warmup_batches)\n",
    "        fps_values.append(fps)\n",
    "        latency_values.append(latency)\n",
    "\n",
    "    # Calcular el promedio y la desviación estándar\n",
    "    avg_fps = np.mean(fps_values)\n",
    "    std_fps = np.std(fps_values)\n",
    "    avg_latency = np.mean(latency_values)\n",
    "    std_latency = np.std(latency_values)\n",
    "\n",
    "    return avg_fps, std_fps, avg_latency, std_latency"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
