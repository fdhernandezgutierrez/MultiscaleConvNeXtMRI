{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c054ca-ac34-4afc-bd5f-9d15b1475116",
   "metadata": {
    "id": "20c054ca-ac34-4afc-bd5f-9d15b1475116"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "import wandb\n",
    "\n",
    "class UNetTrainer:\n",
    "    def __init__(self, model, device, optimizer, criterion, train_loader, val_loader=None, test_loader=None,\n",
    "                 lr_step_size=10, lr_gamma=0.1, epochs=10, filename_model = 'best_model.pth', folder_pretrained= 'pretrained_model/'):\n",
    "        \"\"\"\n",
    "        UNet Trainer class that handles training, validation, and testing.\n",
    "\n",
    "        Args:\n",
    "        - model: The UNet model to be trained.\n",
    "        - device: The computing device (CPU or GPU).\n",
    "        - optimizer: The optimizer to use (SGD, Adam, etc.).\n",
    "        - criterion: The loss function.\n",
    "        - train_loader: DataLoader for training.\n",
    "        - val_loader: DataLoader for validation (optional).\n",
    "        - test_loader: DataLoader for testing (optional).\n",
    "        - lr_step_size: Step size for learning rate scheduler (if applicable).\n",
    "        - lr_gamma: Decay factor for learning rate scheduler.\n",
    "        - epochs: Number of training epochs.\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.epochs = epochs\n",
    "        self.lr_step_size = lr_step_size\n",
    "        self.lr_gamma = lr_gamma\n",
    "        self.filename_model = filename_model\n",
    "        self.folder_pretrained = folder_pretrained\n",
    "        # Learning rate scheduler only for SGD\n",
    "        self.scheduler = None\n",
    "        if isinstance(self.optimizer, optim.SGD):\n",
    "            self.scheduler = CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, step_size_up=lr_step_size, mode='triangular')\n",
    "\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_model_wts = self.model.state_dict()\n",
    "        # Save the losses\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Trains the UNet model with validation.\"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_loss = self._train_one_epoch()\n",
    "\n",
    "            # Validation phase (if applicable)\n",
    "            val_loss = self._validate() if self.val_loader else None\n",
    "\n",
    "            # Print losses and update scheduler if needed\n",
    "            if val_loss is not None:\n",
    "                print(f\"Epoch [{epoch+1}/{self.epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch [{epoch+1}/{self.epochs}] - Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "                print(f\"Learning Rate: {self.scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "            # Append losses to the lists\n",
    "            self.train_losses.append(train_loss)   # code added in 6/08/25\n",
    "            if val_loss is not None:               #code added 6/08/25\n",
    "                self.val_losses.append(val_loss)   #code added 6/08/25\n",
    "                \n",
    "            # Save best model\n",
    "            if val_loss is not None and val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_model_wts = self.model.state_dict()\n",
    "                torch.save(self.best_model_wts, self.folder_pretrained + self.filename_model)\n",
    "                #wandb.save(self.folder_pretrained + self.filename_model) # Testing phase\n",
    "                print(f\"New best model saved with Val Loss: {self.best_val_loss:.4f}\")\n",
    "\n",
    "            # Log to Weights & Biases (wandb)\n",
    "            wandb.log({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss})\n",
    "\n",
    "        print(\"Training completed.\")\n",
    "        self.model.load_state_dict(self.best_model_wts)\n",
    "        print(\"Loaded best model weights.\")\n",
    "\n",
    "    def _train_one_epoch(self):\n",
    "        \"\"\"Runs one epoch of training.\"\"\"\n",
    "        running_loss = 0.0\n",
    "        for images, masks in self.train_loader:\n",
    "            images, masks = images.to(self.device), masks.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        return running_loss / len(self.train_loader)\n",
    "\n",
    "    def _validate(self):\n",
    "        \"\"\"Runs validation.\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in self.val_loader:\n",
    "                images, masks = images.to(self.device), masks.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, masks)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        return running_loss / len(self.val_loader)\n",
    "\n",
    "    def plot_losses(self):   # Function added on 06/08/2025\n",
    "            \"\"\"Plots the training and validation losses.\"\"\"\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(self.train_losses, label='Training Loss', color='blue', linestyle='-', marker='o')\n",
    "            if self.val_loader:\n",
    "                plt.plot(self.val_losses, label='Validation Loss', color='red', linestyle='-', marker='x')\n",
    "            plt.title('Training and Validation Losses')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        \n",
    "    def test(self):\n",
    "        \"\"\"Runs the model on the test set and returns predictions.\"\"\"\n",
    "        if not self.test_loader:\n",
    "            raise ValueError(\"Test DataLoader not provided.\")\n",
    "\n",
    "        self.model.eval()\n",
    "        all_images, all_preds, all_targets = [], [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in self.test_loader:\n",
    "                images, masks = images.to(self.device), masks.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "\n",
    "                all_images.append(images.cpu())\n",
    "                all_preds.append(outputs.cpu())\n",
    "                all_targets.append(masks.cpu())\n",
    "\n",
    "        return torch.cat(all_images, dim=0), torch.cat(all_preds, dim=0), torch.cat(all_targets, dim=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_model_stats(model, input_tensor):\n",
    "        \"\"\"\n",
    "        Computes FLOPs, total parameters, and trainable parameters of the model.\n",
    "\n",
    "        Returns:\n",
    "        - flops (int): Total number of FLOPs for a forward pass.\n",
    "        - total_params (int): Total number of parameters in the model.\n",
    "        - trainable_params (int): Number of trainable parameters in the model.\n",
    "        \"\"\"\n",
    "        # Compute FLOPs\n",
    "        flop_count = FlopCountAnalysis(model, input_tensor)\n",
    "\n",
    "        # Compute total and trainable parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "        #print(f\"FLOPs: {flop_count.total():,}\")  # Print FLOPs with formatting\n",
    "        print(f\"Total Parameters: {total_params:,}\")\n",
    "        print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "        return flop_count, total_params, trainable_params\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_results(images, targets, preds, num_samples=3, save_path=None):\n",
    "        if save_path is not None:\n",
    "            pdf = PdfPages(save_path)\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            image = images[i].numpy()\n",
    "            target = targets[i].squeeze(0).numpy()\n",
    "            pred = preds[i].squeeze(0).numpy()\n",
    "            pred_binary = (pred > 0.5).astype(np.uint8)\n",
    "        \n",
    "            # Ajustar visualizaci√≥n RGB o grayscale\n",
    "            if image.shape[0] == 1:\n",
    "                image = image.squeeze(0)\n",
    "            elif image.shape[0] == 3:\n",
    "                image = image.transpose(1, 2, 0)\n",
    "        \n",
    "            fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            axs[0].imshow(image, cmap=\"gray\" if image.ndim == 2 else None)\n",
    "            axs[0].set_title(\"Original Image\")\n",
    "            axs[0].axis('off')\n",
    "        \n",
    "            axs[1].imshow(target, cmap=\"gray\")\n",
    "            axs[1].set_title(\"Ground Truth\")\n",
    "            axs[1].axis('off')\n",
    "        \n",
    "            axs[2].imshow(pred_binary, cmap=\"gray\")\n",
    "            axs[2].set_title(\"Prediction\")\n",
    "            axs[2].axis('off')\n",
    "        \n",
    "            plt.tight_layout()\n",
    "        \n",
    "            if save_path is not None:\n",
    "                pdf.savefig(fig)  # Guardar figura en PDF\n",
    "                plt.close(fig)    # Cerrar figura para liberar memoria\n",
    "            else:\n",
    "                plt.show()\n",
    "        \n",
    "        if save_path is not None:\n",
    "            pdf.close()\n",
    "            print(f\"Resultados guardados en PDF: {save_path}\")\n",
    "            wandb.save(save_path)\n",
    "\n",
    "    def save_model_wanDB(self):\n",
    "        torch.save(self.model.state_dict(), self.folder_pretrained+ self.filename_model)\n",
    "        # Subir el modelo a WandB\n",
    "        wandb.save(self.folder_pretrained+ self.filename_model)\n",
    "        print(\"Model saved and uploaded to WandB\")\n",
    "\n",
    "    def evaluate_metrics(self, threshold=0.1):\n",
    "            \"\"\"\n",
    "            Evaluates the model and logs the metrics to WandB.\n",
    "            \"\"\"\n",
    "            evaluator = SegmentationEvaluator(self.model, self.test_loader, threshold, device=self.device)\n",
    "            metrics = evaluator.evaluate(visualize=True)\n",
    "            print(metrics)\n",
    "            wandb.log(metrics)  # Log the metrics to WandB\n",
    "            return metrics"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
