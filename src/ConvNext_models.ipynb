{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXzH2VWN5vcI"
   },
   "source": [
    "# ConvNeXt using backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ej1EsqEz5i1h"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Cargar ConvNeXt como backbone\n",
    "#backbone = models.convnext_base(pretrained=True).features  # Solo las características, version anteriro. \n",
    "backbone = models.convnext_base(weights=models.ConvNeXt_Base_Weights.DEFAULT).features  # only features\n",
    "\n",
    "# Cambiar la primera capa convolucional para aceptar un canal en lugar de tres\n",
    "backbone[0][0] = nn.Conv2d(1, backbone[0][0].out_channels, kernel_size=4, stride=4, padding=0)\n",
    "\n",
    "# Definir un modelo de segmentación usando ConvNeXt como backbone\n",
    "class ConvNeXtSegmentation(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeXtSegmentation, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        # Decodificador ajustado para obtener una salida de 240x240\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),\n",
    "            nn.GELU(),#ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(64, num_classes, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNext Base with Skip connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_ch, skip_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_ch + skip_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        if x.shape[-2:] != skip.shape[-2:]:          # corrige off-by-1\n",
    "            x = torch.nn.functional.interpolate(\n",
    "                x, size=skip.shape[-2:], mode=\"bilinear\",\n",
    "                align_corners=False)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# ConvNeXt-Base como encoder + decoder tipo U-Net con skips\n",
    "class ConvNeXtUNet_B(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # -------------------- Encoder ----------------------------------\n",
    "        convnext = models.convnext_base(\n",
    "            weights=models.ConvNeXt_Base_Weights.DEFAULT).features\n",
    "\n",
    "        # aceptar imágenes de 1 canal\n",
    "        convnext[0][0] = nn.Conv2d(\n",
    "            1, convnext[0][0].out_channels,\n",
    "            kernel_size=4, stride=4, padding=0, bias=False)\n",
    "\n",
    "        # separar etapas para capturar los mapas de características\n",
    "        self.enc_stem   = convnext[0]   # 1/4 , 128 ch\n",
    "        self.enc_stage0 = convnext[1]   # 1/4 , 128 ch\n",
    "        self.down1      = convnext[2]   # ↓ 1/8\n",
    "        self.enc_stage1 = convnext[3]   # 1/8 , 256 ch\n",
    "        self.down2      = convnext[4]   # ↓ 1/16\n",
    "        self.enc_stage2 = convnext[5]   # 1/16, 512 ch\n",
    "        self.down3      = convnext[6]   # ↓ 1/32\n",
    "        self.enc_stage3 = convnext[7]   # 1/32, 1024 ch\n",
    "\n",
    "        # el LayerNorm final sólo existe si len(convnext) > 8\n",
    "        self.enc_norm = convnext[8] if len(convnext) > 8 else nn.Identity()\n",
    "\n",
    "        # -------------------- Decoder ----------------------------------\n",
    "        self.up3  = UpBlock(1024, 512, 512)   # 1/32 → 1/16\n",
    "        self.up2  = UpBlock(512, 256, 256)    # 1/16 → 1/8\n",
    "        self.up1  = UpBlock(256, 128, 128)    # 1/8  → 1/4\n",
    "        self.up0  = nn.Sequential(            # 1/4  → 1\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=4),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.head = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def forward(self, x):\n",
    "        # ---------- Encoder -------------\n",
    "        x0 = self.enc_stage0(self.enc_stem(x))      # 1/4\n",
    "        x1 = self.enc_stage1(self.down1(x0))        # 1/8\n",
    "        x2 = self.enc_stage2(self.down2(x1))        # 1/16\n",
    "        x3 = self.enc_stage3(self.down3(x2))        # 1/32\n",
    "        x3 = self.enc_norm(x3)                      # LN o identidad\n",
    "\n",
    "        # ---------- Decoder -------------\n",
    "        d2 = self.up3(x3, x2)   # 1/16\n",
    "        d1 = self.up2(d2, x1)   # 1/8\n",
    "        d0 = self.up1(d1, x0)   # 1/4\n",
    "        d0 = self.up0(d0)       # 1\n",
    "        out = self.head(d0)     # logits\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNext variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Utilidad: crea backbone ConvNeXt (cualquier variante)\n",
    "# --------------------------------------------------\n",
    "def get_convnext_features(variant=\"base\", in_ch=1):\n",
    "    fn  = dict(base = models.convnext_base,\n",
    "               tiny = models.convnext_tiny,\n",
    "               small= models.convnext_small,\n",
    "               large= models.convnext_large)[variant]\n",
    "    w   = getattr(models, f\"ConvNeXt_{variant.capitalize()}_Weights\").DEFAULT\n",
    "    enc = fn(weights=w).features          # solo stages ConvNeXt\n",
    "    enc[0][0] = nn.Conv2d(in_ch, enc[0][0].out_channels,\n",
    "                          kernel_size=4, stride=4, padding=0)\n",
    "    return enc\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Bloque Reverse-Attention sencillo\n",
    "# --------------------------------------------------\n",
    "class ReverseAttention(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(ch, ch, 3, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(ch)\n",
    "        self.conv2 = nn.Conv2d(ch, ch, 3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        m = torch.sigmoid(self.bn2(self.conv2(F.gelu(self.bn1(self.conv1(x))))))\n",
    "        return x * (1.0 - m) + x               # “reverse”: suprime píxeles poco relevantes\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Bloque Multi-Feature Stem (k3,k5,k7, stride=4)\n",
    "# --------------------------------------------------\n",
    "class MultiFeatureStem(nn.Module):\n",
    "    def __init__(self, out_ch, in_ch=1):\n",
    "        super().__init__()\n",
    "        split = out_ch // 3\n",
    "        cfg   = [split, split, out_ch - 2*split]\n",
    "        ks_pd = [(3,1), (5,2), (7,3)]\n",
    "        self.branches = nn.ModuleList(\n",
    "            [nn.Conv2d(in_ch, cfg[i], k, stride=4, padding=p, bias=False)\n",
    "             for i,(k,p) in enumerate(ks_pd)]\n",
    "        )\n",
    "        self.bn  = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([b(x) for b in self.branches], 1)\n",
    "        return F.gelu(self.bn(x))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Modelo 2 : Reverse-Attention en la salida\n",
    "# --------------------------------------------------\n",
    "class ConvNeXtSegmentationRA(ConvNeXtSegmentation): # ConvNeXtSegmentation hierarchy\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__(num_classes)\n",
    "        # ponemos RA justo antes del último upsample (64 canales)\n",
    "        self.ra = ReverseAttention(64)\n",
    "\n",
    "        # sustituimos los dos últimos bloques del decoder\n",
    "        *head, last2, last1 = self.decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            *head,                        # hasta 128→64\n",
    "            nn.Sequential(                # RA + upsample 64→240\n",
    "                self.ra,\n",
    "                last2\n",
    "            ),\n",
    "            last1                         # 64/num_classes upsample final\n",
    "        )\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Modelo 3 : Multi-Feature Stem + ConvNeXt\n",
    "# --------------------------------------------------\n",
    "\n",
    "#class ConvNeXtSegmentationMF(nn.Module): # ConvNeXtSegmentationMF\n",
    "#    def __init__(self, num_classes=1):\n",
    "#        super().__init__()\n",
    "#        # creamos stem y backbone (omitimos patch-embedding interno)\n",
    "#        tmp_backbone = get_convnext_features(\"base\", in_ch=1)\n",
    "#        first_out_ch = tmp_backbone[0][0].out_channels\n",
    "#        self.stem     = MultiFeatureStem(first_out_ch, in_ch=1)\n",
    "#        self.backbone = nn.Sequential(*tmp_backbone[1:])  # quitamos capa 0 (ya cubierta)\n",
    "\n",
    "#        # mismo decoder que el modelo base\n",
    "#        self.decoder  = ConvNeXtSegmentation(num_classes).decoder\n",
    "\n",
    "#    def forward(self, x):\n",
    "#        x = self.stem(x)\n",
    "#        x = self.backbone(x)\n",
    "#        return self.decoder(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4WRdcy_62Tj",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ConvNet with BackBone and SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backbone = models.convnext_base(weights=models.ConvNeXt_Base_Weights.DEFAULT).features  # Only features\n",
    "# Cambiar la primera capa convolucional para aceptar un canal en lugar de tres\n",
    "#backbone[0][0] = nn.Conv2d(1, backbone[0][0].out_channels, kernel_size=4, stride=4, padding=0)\n",
    "# Módulo de atención Squeeze-and-Excitation (SE)\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = x.mean(dim=(2, 3), keepdim=True)  # Global average pooling\n",
    "        y = y.view(b, c)\n",
    "        y = self.fc1(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.sigmoid(y).view(b, c, 1, 1)\n",
    "        return x * y  # Element-wise multiplication with input\n",
    "\n",
    "# Definir un modelo de segmentación usando ConvNeXt como backbone\n",
    "class ConvNeXtSegmentation_SE(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeXtSegmentation_SE, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        \n",
    "        # Agregar bloque de atención Squeeze-and-Excitation\n",
    "        self.attention = SEBlock(1024)\n",
    "\n",
    "        # Decodificador ajustado para obtener una salida de 240x240\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(64, num_classes, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.attention(x)  # Aplicar la atención después del backbone\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ConvNeXt with backbone and Reverse Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar ConvNeXt como backbone\n",
    "backbone = models.convnext_base(weights=models.ConvNeXt_Base_Weights.DEFAULT).features  # Only features\n",
    "\n",
    "# Cambiar la primera capa convolucional para aceptar un canal en lugar de tres\n",
    "backbone[0][0] = nn.Conv2d(1, backbone[0][0].out_channels, kernel_size=4, stride=4, padding=0)\n",
    "\n",
    "# Bloque de Reverse Attention\n",
    "class ReverseAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ReverseAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(channels, channels // 2, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(channels, channels // 2, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generar los mapas de consulta, clave y valor\n",
    "        query = self.query_conv(x).view(x.size(0), -1, x.size(2) * x.size(3))\n",
    "        key = self.key_conv(x).view(x.size(0), -1, x.size(2) * x.size(3))\n",
    "        value = self.value_conv(x).view(x.size(0), -1, x.size(2) * x.size(3))\n",
    "        \n",
    "        # Calcular la atención \"reverse\" (inversa), es decir, invertimos el enfoque habitual\n",
    "        attention = torch.bmm(query.permute(0, 2, 1), key)  # Producto punto entre consulta y clave\n",
    "        attention = 1 / (1 + torch.exp(-attention))  # Activación sigmoid para \"invertir\" la atención\n",
    "        \n",
    "        # Aplicar la atención a los valores\n",
    "        out = torch.bmm(value, attention)\n",
    "        out = out.view(x.size(0), x.size(1), x.size(2), x.size(3))  # Redimensionar de vuelta\n",
    "        return out\n",
    "\n",
    "# Definir un modelo de segmentación usando ConvNeXt como backbone\n",
    "class ConvNeXtSegmentation_RA(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeXtSegmentation_RA, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        \n",
    "        # Agregar bloque de Reverse Attention\n",
    "        self.reverse_attention = ReverseAttention(1024)\n",
    "\n",
    "        # Decodificador ajustado para obtener una salida de 240x240\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(64, num_classes, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.reverse_attention(x)  # Aplicar el reverse attention\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ConvNextUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "class ConvNeXtUNet(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1, backbone_type: str = \"base\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # ------------------- Backbones disponibles -------------------\n",
    "        cfg = {\n",
    "            \"base\":  dict(fn=models.convnext_base,  w=models.ConvNeXt_Base_Weights.DEFAULT,\n",
    "                          ch=[128, 256, 512, 1024]),\n",
    "            \"large\": dict(fn=models.convnext_large, w=models.ConvNeXt_Large_Weights.DEFAULT,\n",
    "                          ch=[192, 384, 768, 1536]),\n",
    "            \"tiny\":  dict(fn=models.convnext_tiny,  w=models.ConvNeXt_Tiny_Weights.DEFAULT,\n",
    "                          ch=[ 96, 192, 384,  768]),\n",
    "            \"small\": dict(fn=models.convnext_small, w=models.ConvNeXt_Small_Weights.DEFAULT,\n",
    "                          ch=[ 96, 192, 384,  768]),\n",
    "        }\n",
    "        assert backbone_type in cfg, f\"Backbone «{backbone_type}» no soportado.\"\n",
    "        c = cfg[backbone_type]\n",
    "\n",
    "        # ------------------- Encoder ConvNeXt -----------------------\n",
    "        self.backbone = c[\"fn\"](weights=c[\"w\"]).features\n",
    "        # 1 canal de entrada, stride=4 (512→128)\n",
    "        self.backbone[0][0] = nn.Conv2d(1, c[\"ch\"][0], kernel_size=4, stride=4, padding=0)\n",
    "\n",
    "        # ------------------- Decoder (×2 en cada etapa) --------------\n",
    "        self.up4 = self._up_block(c[\"ch\"][3]    , c[\"ch\"][2])       # 16 → 32\n",
    "        self.up3 = self._up_block(c[\"ch\"][2] * 2, c[\"ch\"][1])       # 32 → 64\n",
    "        self.up2 = self._up_block(c[\"ch\"][1] * 2, c[\"ch\"][0])       # 64 →128\n",
    "        self.up1 = self._up_block(c[\"ch\"][0] * 2, 64)               #128 →256\n",
    "        self.up0 = self._up_block(64, 32)                           #256 →512\n",
    "\n",
    "        self.final_conv = nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "\n",
    "    # ---------- Bloque upsample (ConvT2d + conv 3×3) ----------------\n",
    "    @staticmethod\n",
    "    def _up_block(in_c: int, out_c: int):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    # ---------- Asegura que dos tensores tengan mismo H×W -----------\n",
    "    @staticmethod\n",
    "    def _match(x, ref):\n",
    "        if x.shape[2:] != ref.shape[2:]:\n",
    "            x = F.interpolate(x, size=ref.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        return x\n",
    "\n",
    "    # ---------------------------- Forward ---------------------------\n",
    "    def forward(self, x):\n",
    "        skips, prev = [], None\n",
    "        for layer in self.backbone:\n",
    "            x = layer(x)\n",
    "            # guardamos solo cuando cambia la resolución\n",
    "            if prev is None or x.shape[-1] != prev:\n",
    "                skips.append(x)\n",
    "                prev = x.shape[-1]\n",
    "\n",
    "        # Esperamos 4 escalas (128,64,32,16) → comprobación\n",
    "        assert len(skips) >= 4, \"No se capturaron suficientes escalas del encoder.\"\n",
    "\n",
    "        # Decoder\n",
    "        x = self.up4(skips[-1])                                  # 16→32\n",
    "        x = torch.cat([self._match(x, skips[-2]), skips[-2]], 1)\n",
    "\n",
    "        x = self.up3(x)                                          # 32→64\n",
    "        x = torch.cat([self._match(x, skips[-3]), skips[-3]], 1)\n",
    "\n",
    "        x = self.up2(x)                                          # 64→128\n",
    "        x = torch.cat([self._match(x, skips[-4]), skips[-4]], 1)\n",
    "\n",
    "        x = self.up1(x)                                          #128→256\n",
    "        x = self.up0(x)                                          #256→512\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Reverse-Attention module\n",
    "# ----------------------------\n",
    "class SkipReverseAttn(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1   = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.bn1     = nn.BatchNorm2d(channels)\n",
    "        self.act     = nn.GELU()\n",
    "        self.conv2   = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.bn2     = nn.BatchNorm2d(channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn = self.act(self.bn1(self.conv1(x)))\n",
    "        attn = self.sigmoid(self.bn2(self.conv2(attn)))\n",
    "        return x * (1.0 - attn) + x\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2) ConvNeXt-UNet + Reverse-Attention\n",
    "# -----------------------------------------\n",
    "class ConvNeXtRAUNet(nn.Module):\n",
    "    def __init__(self, num_classes=1, backbone_type='base'):\n",
    "        super().__init__()\n",
    "\n",
    "        cfg = {\n",
    "            'tiny' : dict(fn=models.convnext_tiny ,  w=models.ConvNeXt_Tiny_Weights.DEFAULT ,\n",
    "                          ch=[ 96, 192, 384,  768]),\n",
    "            'small': dict(fn=models.convnext_small,  w=models.ConvNeXt_Small_Weights.DEFAULT,\n",
    "                          ch=[ 96, 192, 384,  768]),\n",
    "            'base' : dict(fn=models.convnext_base ,  w=models.ConvNeXt_Base_Weights.DEFAULT ,\n",
    "                          ch=[128, 256, 512, 1024]),\n",
    "            'large': dict(fn=models.convnext_large,  w=models.ConvNeXt_Large_Weights.DEFAULT,\n",
    "                          ch=[192, 384, 768, 1536])\n",
    "        }\n",
    "        assert backbone_type in cfg, f\"Backbone '{backbone_type}' no soportado.\"\n",
    "        c = cfg[backbone_type]\n",
    "\n",
    "        # Encoder\n",
    "        self.backbone = c['fn'](weights=c['w']).features\n",
    "        self.backbone[0][0] = nn.Conv2d(1, c['ch'][0], kernel_size=4, stride=4, padding=0)\n",
    "\n",
    "        # Reverse-Attention en skips\n",
    "        self.ra3 = SkipReverseAttn(c['ch'][2])\n",
    "        self.ra2 = SkipReverseAttn(c['ch'][1])\n",
    "        self.ra1 = SkipReverseAttn(c['ch'][0])\n",
    "\n",
    "        # Decoder\n",
    "        self.up4 = self._up_block(c['ch'][3]      , c['ch'][2])\n",
    "        self.up3 = self._up_block(c['ch'][2]*2    , c['ch'][1])\n",
    "        self.up2 = self._up_block(c['ch'][1]*2    , c['ch'][0])\n",
    "        self.up1 = self._up_block(c['ch'][0]*2    , 64)\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def _up_block(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for layer in self.backbone:\n",
    "            x = layer(x)\n",
    "            skips.append(x)\n",
    "\n",
    "        x = self.up4(skips[-1])\n",
    "        x = torch.cat([x, self.ra3(skips[-2])], dim=1)\n",
    "\n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x, self.ra2(skips[-3])], dim=1)\n",
    "\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, self.ra1(skips[-4])], dim=1)\n",
    "\n",
    "        x = self.up1(x)\n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNext UNet + MultiFeatures + Skip Reverse Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1)  Multi-Feature Stem  (tres ramas: k3/k5/k7, s=4)\n",
    "# --------------------------------------------------\n",
    "class MultiFeatureStem(nn.Module):\n",
    "    def __init__(self, *, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        split  = out_ch // 3\n",
    "        ch_cfg = [split, split, out_ch - 2 * split]   # suma == out_ch\n",
    "        ks_pad = [(3, 1), (5, 2), (7, 3)]\n",
    "\n",
    "        self.branches = nn.ModuleList(\n",
    "            nn.Conv2d(in_ch, ch_cfg[i], kernel_size=k, stride=4, padding=p, bias=False)\n",
    "            for i, (k, p) in enumerate(ks_pad)\n",
    "        )\n",
    "        self.bn  = nn.BatchNorm2d(out_ch)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([b(x) for b in self.branches], dim=1)\n",
    "        return self.act(self.bn(x))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2)  Reverse-Attention para los skips\n",
    "# --------------------------------------------------\n",
    "class SkipReverseAttn(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(channels)\n",
    "        self.act   = nn.GELU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(channels)\n",
    "        self.sig   = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        m = self.act(self.bn1(self.conv1(x)))\n",
    "        m = self.sig(self.bn2(self.conv2(m)))\n",
    "        return x * (1.0 - m) + x\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3)  ConvNeXt-UNet + MF + RA   (salida H×W)\n",
    "# --------------------------------------------------\n",
    "class ConvNeXtSegmentationMF(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1, backbone_type: str = \"base\"):\n",
    "        super().__init__()\n",
    "\n",
    "        cfg = {\n",
    "            \"tiny\":  dict(fn=models.convnext_tiny , w=models.ConvNeXt_Tiny_Weights.DEFAULT ,\n",
    "                          ch=[ 96, 192, 384,  768]),\n",
    "            \"small\": dict(fn=models.convnext_small, w=models.ConvNeXt_Small_Weights.DEFAULT,\n",
    "                          ch=[ 96, 192, 384,  768]),\n",
    "            \"base\":  dict(fn=models.convnext_base , w=models.ConvNeXt_Base_Weights.DEFAULT ,\n",
    "                          ch=[128, 256, 512, 1024]),\n",
    "            \"large\": dict(fn=models.convnext_large, w=models.ConvNeXt_Large_Weights.DEFAULT,\n",
    "                          ch=[192, 384, 768, 1536]),\n",
    "        }\n",
    "        assert backbone_type in cfg, \"Backbone no soportado.\"\n",
    "        self.ch = cfg[backbone_type][\"ch\"]\n",
    "\n",
    "        # ---------- Stem ----------\n",
    "        self.stem = MultiFeatureStem(in_ch=1, out_ch=self.ch[0])\n",
    "\n",
    "        # ---------- Encoder (quitamos patch-embed) ----------\n",
    "        enc_full = cfg[backbone_type][\"fn\"](weights=cfg[backbone_type][\"w\"]).features\n",
    "        self.encoder = nn.Sequential(*enc_full[1:])   # sin capa 0\n",
    "\n",
    "        # ---------- Reverse-Attention ----------\n",
    "        self.ra3 = SkipReverseAttn(self.ch[2])\n",
    "        self.ra2 = SkipReverseAttn(self.ch[1])\n",
    "        self.ra1 = SkipReverseAttn(self.ch[0])\n",
    "\n",
    "        # ---------- Decoder (H/32 → H) ----------\n",
    "        self.up4 = self._up(self.ch[3]    , self.ch[2])   # H/32 → H/16\n",
    "        self.up3 = self._up(self.ch[2]*2 , self.ch[1])   # H/16 → H/8\n",
    "        self.up2 = self._up(self.ch[1]*2 , self.ch[0])   # H/8  → H/4\n",
    "        self.up1 = self._up(self.ch[0]*2 , 64)           # H/4  → H/2\n",
    "        self.up0 = self._up(64           , 32)           # H/2  → H\n",
    "        self.final_conv = nn.Conv2d(32, num_classes, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _up(in_ch: int, out_ch: int) -> nn.Sequential:\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    # ---------- Forward ----------\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)                #  H/4\n",
    "        skip1 = x                       # 128ch\n",
    "\n",
    "        skip2 = skip3 = None\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            c = x.shape[1]\n",
    "            if c == self.ch[1]:   # 256ch  (H/8)\n",
    "                skip2 = x\n",
    "            elif c == self.ch[2]: # 512ch  (H/16)\n",
    "                skip3 = x\n",
    "\n",
    "        # Bottleneck: 1024ch, H/32\n",
    "        x = self.up4(x)                            # 512ch, H/16\n",
    "        x = torch.cat([x, self.ra3(skip3)], dim=1) # 1024ch\n",
    "\n",
    "        x = self.up3(x)                            # 256ch, H/8\n",
    "        x = torch.cat([x, self.ra2(skip2)], dim=1) # 512ch\n",
    "\n",
    "        x = self.up2(x)                            # 128ch, H/4\n",
    "        x = torch.cat([x, self.ra1(skip1)], dim=1) # 256ch\n",
    "\n",
    "        x = self.up1(x)                            # 64ch , H/2\n",
    "        x = self.up0(x)                            # 32ch , H\n",
    "        return self.final_conv(x)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4)  Prueba de forma\n",
    "# --------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    model = ConvNeXtSegmentationMF(num_classes=1, backbone_type=\"base\")\n",
    "    x = torch.randn(2, 1, 512, 512)\n",
    "    y = model(x)\n",
    "    print(\"Output:\", y.shape)          # torch.Size([2, 1, 512, 512])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperNet model usign ConvNext and attention modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Módulo de atención para resaltar las características más relevantes de la imagen.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "\n",
    "        # Calcular la atención\n",
    "        query = self.query_conv(x).view(batch_size, -1, height * width).permute(0, 2, 1)\n",
    "        key = self.key_conv(x).view(batch_size, -1, height * width)\n",
    "        energy = torch.bmm(query, key)\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        # Aplicar atención a las características\n",
    "        value = self.value_conv(x).view(batch_size, -1, height * width)\n",
    "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, channels, height, width)\n",
    "\n",
    "        # Salida final con atención\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "class HyperNetSegmentation(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo de segmentación \"HyperNet\" que combina ConvNeXt, atención y fusión profunda de características.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(HyperNetSegmentation, self).__init__()\n",
    "        \n",
    "        # Encoder con ConvNeXt (construcción de una arquitectura similar a ConvNeXt)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Módulos de atención para resaltar las regiones relevantes\n",
    "        self.attn1 = AttentionBlock(64)  # Atención para la primera capa\n",
    "        self.attn2 = AttentionBlock(128)  # Atención para la segunda capa\n",
    "        self.attn3 = AttentionBlock(256)  # Atención para la tercera capa\n",
    "        self.attn4 = AttentionBlock(512)  # Atención para la cuarta capa\n",
    "\n",
    "        # Decoder (fusión de características profundas)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, num_classes, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pasa por el encoder\n",
    "        x1 = self.encoder[0:2](x)\n",
    "        x2 = self.encoder[2:4](x1)\n",
    "        x3 = self.encoder[4:6](x2)\n",
    "        x4 = self.encoder[6:](x3)\n",
    "\n",
    "        # Aplicar atención\n",
    "        x1 = self.attn1(x1)\n",
    "        x2 = self.attn2(x2)\n",
    "        x3 = self.attn3(x3)\n",
    "        x4 = self.attn4(x4)\n",
    "\n",
    "        # Fusionar características (la idea es combinar características de diferentes niveles)\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "\n",
    "        # Pasar por el decoder\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        # Para segmentación binaria, aplicar sigmoide en la salida\n",
    "        x = torch.sigmoid(x)  # Probabilidad en el rango [0, 1]\n",
    "        return x\n",
    "\n",
    "# Definir el modelo\n",
    "#model = HyperNetSegmentation(num_classes=1)  # Para segmentación binaria"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPs1Nhji/Ah6/Ld86Mter+1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
