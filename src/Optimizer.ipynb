{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef09590-c043-46fa-a303-4fec8b9ee756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class CRTAdamW(Optimizer):\n",
    "    r\"\"\"\n",
    "    Centralized-Rectified Time-decay AdamW (CRT-AdamW)\n",
    "\n",
    "    Args:\n",
    "        params (iterable): model parameters.\n",
    "        lr (float): learning-rate η.\n",
    "        betas (Tuple[float,float]): (β1, β2).\n",
    "        eps (float): ε for numerical stability.\n",
    "        weight_decay (float): initial λ₀.\n",
    "        decay_alpha (float): α in λ_t = λ₀ / (1 + α t)^β.\n",
    "        decay_beta  (float): β in λ_t = λ₀ / (1 + α t)^β.\n",
    "        gc_conv_only (bool): if True, apply Gradient Centralization only to\n",
    "                             parameters with dim > 1 (recommended).\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999),\n",
    "                 eps=1e-8, weight_decay=1e-2,\n",
    "                 decay_alpha=1e-4, decay_beta=0.5,\n",
    "                 gc_conv_only=True):\n",
    "        if lr <= 0.0:\n",
    "            raise ValueError(f\"Invalid lr: {lr}\")\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay,\n",
    "                        decay_alpha=decay_alpha, decay_beta=decay_beta,\n",
    "                        gc_conv_only=gc_conv_only)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            beta1, beta2 = group['betas']\n",
    "            lr, eps = group['lr'], group['eps']\n",
    "            lam0 = group['weight_decay']\n",
    "            alpha, beta = group['decay_alpha'], group['decay_beta']\n",
    "            gc_conv_only = group['gc_conv_only']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad\n",
    "\n",
    "                # --------- Gradient Centralization -------------\n",
    "                if grad.ndim > 1 or not gc_conv_only:\n",
    "                    grad = grad - grad.mean(dim=tuple(range(1, grad.ndim)), keepdim=True)\n",
    "\n",
    "                state = self.state[p]\n",
    "                if not state:   # state initialization\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                state['step'] += 1\n",
    "                t = state['step']\n",
    "\n",
    "                # moments\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                # bias corrections\n",
    "                bias_c1 = 1 - beta1 ** t\n",
    "                bias_c2 = 1 - beta2 ** t\n",
    "                m_hat = exp_avg / bias_c1\n",
    "                v_hat = exp_avg_sq / bias_c2\n",
    "\n",
    "                # ---------- Rectification (RAdam) ---------------\n",
    "                rho_inf = 2 / (1 - beta2) - 1\n",
    "                rho_t = rho_inf - 2 * t * (beta2 ** t) / (1 - beta2 ** t)\n",
    "                if rho_t > 4:\n",
    "                    r_t = math.sqrt((rho_t - 4) * (rho_t - 2) * rho_inf /\n",
    "                                    ((rho_inf - 4) * (rho_inf - 2) * rho_t))\n",
    "                    update = m_hat * r_t / (torch.sqrt(v_hat) + eps)\n",
    "                else:\n",
    "                    update = m_hat\n",
    "\n",
    "                # ----------- Time-decay weight-decay ------------\n",
    "                lam_t = lam0 / (1.0 + alpha * t) ** beta\n",
    "\n",
    "                # parameter update\n",
    "                p.add_(update, alpha=-lr)\n",
    "                if lam_t != 0.0:\n",
    "                    p.add_(p, alpha=-lr * lam_t)\n",
    "\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
